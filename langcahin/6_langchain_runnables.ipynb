{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a090d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c324546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NakliLLM:\n",
    "\n",
    "  def __init__(self):\n",
    "    print('LLM created')\n",
    "\n",
    "  def predict(self, prompt):\n",
    "\n",
    "    response_list = [\n",
    "        'Delhi is the capital of India',\n",
    "        'IPL is a cricket league',\n",
    "        'AI stands for Artificial Intelligence'\n",
    "    ]\n",
    "\n",
    "    return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0a8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliPromptTemplate:\n",
    "\n",
    "  def __init__(self, template, input_variables):\n",
    "    self.template = template\n",
    "    self.input_variables = input_variables\n",
    "\n",
    "  def format(self, input_dict):\n",
    "    return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4944d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492cfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format({'length':'short','topic':'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7df3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7cb2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'AI stands for Artificial Intelligence'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1629259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliLLMChain:\n",
    "\n",
    "  def __init__(self, llm, prompt):\n",
    "    self.llm = llm\n",
    "    self.prompt = prompt\n",
    "\n",
    "  def run(self, input_dict):\n",
    "\n",
    "    final_prompt = self.prompt.format(input_dict)\n",
    "    result = self.llm.predict(final_prompt)\n",
    "\n",
    "    return result['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd77773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43be027",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = NakliLLMChain(llm, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d75db47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({'length':'short', 'topic': 'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297b44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d597a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6806c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable(ABC):\n",
    "\n",
    "  @abstractmethod\n",
    "  def invoke(input_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc21f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NakliLLM(Runnable):\n",
    "\n",
    "  def __init__(self):\n",
    "    print('LLM created')\n",
    "\n",
    "  def invoke(self, prompt):\n",
    "    response_list = [\n",
    "        'Delhi is the capital of India',\n",
    "        'IPL is a cricket league',\n",
    "        'AI stands for Artificial Intelligence'\n",
    "    ]\n",
    "\n",
    "    return {'response': random.choice(response_list)}\n",
    "\n",
    "\n",
    "  def predict(self, prompt):\n",
    "\n",
    "    response_list = [\n",
    "        'Delhi is the capital of India',\n",
    "        'IPL is a cricket league',\n",
    "        'AI stands for Artificial Intelligence'\n",
    "    ]\n",
    "\n",
    "    return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebee3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliPromptTemplate(Runnable):\n",
    "\n",
    "  def __init__(self, template, input_variables):\n",
    "    self.template = template\n",
    "    self.input_variables = input_variables\n",
    "\n",
    "  def invoke(self, input_dict):\n",
    "    return self.template.format(**input_dict)\n",
    "\n",
    "  def format(self, input_dict):\n",
    "    return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d617605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliStrOutputParser(Runnable):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def invoke(self, input_data):\n",
    "    return input_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e3db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnableConnector(Runnable):\n",
    "\n",
    "  def __init__(self, runnable_list):\n",
    "    self.runnable_list = runnable_list\n",
    "\n",
    "  def invoke(self, input_data):\n",
    "\n",
    "    for runnable in self.runnable_list:\n",
    "      input_data = runnable.invoke(input_data)\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b5a2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b96bf63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()\n",
    "parser = NakliStrOutputParser()\n",
    "chain = RunnableConnector([template, llm, parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc65526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IPL is a cricket league'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'length':'long', 'topic':'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00cfbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = NakliPromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "755012e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = NakliPromptTemplate(\n",
    "    template='Explain the following joke {response}',\n",
    "    input_variables=['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89462f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()\n",
    "parser = NakliStrOutputParser()\n",
    "chain1 = RunnableConnector([template1, llm])\n",
    "chain2 = RunnableConnector([template2, llm, parser])\n",
    "final_chain = RunnableConnector([chain1, chain2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab805159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi is the capital of India'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({'topic':'cricket'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7bd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with langchain runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4cd70",
   "metadata": {},
   "source": [
    "#### Runnable Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c759170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down the joke:\n",
      "\n",
      "*   **The Setup:** The joke begins with the classic \"Why did the chicken cross the road?\" setup. This sets the expectation for a silly or simple punchline.\n",
      "\n",
      "*   **The AI Twist:** The punchline replaces the chicken with an AI, immediately signaling a joke about artificial intelligence.\n",
      "\n",
      "*   **The Algorithm Prediction:** \"Because its algorithm predicted a 99.9% chance of reaching the other side...\" This highlights a core concept of AI: using algorithms to make predictions based on data. The high probability (99.9%) emphasizes the AI's confidence in its calculation. It's a humorous way to suggest that the AI is driven by data and a desire for accurate results.\n",
      "\n",
      "*   **Avoiding Error:** \"...and it didn't want to be wrong.\" This adds to the humor by personifying the AI with a desire to be correct. It plays on the idea that AI, while not having emotions, is programmed to optimize for accuracy. The AI crossed the road not because it \"wanted\" to, but because being wrong is undesirable in its programming.\n",
      "\n",
      "*   **Cat Pictures:** \"Plus, it heard there was a new dataset of cat pictures over there.\" This is the final layer of the joke. AI, especially in the realm of image recognition, is often trained on massive datasets. Cat pictures are a common and almost stereotypical example of such datasets. The joke implies that the AI's \"curiosity\" (or, more accurately, its programming) is driven by the prospect of acquiring more data for learning and improvement. It's a lighthearted jab at how AI is often focused on specific tasks and data.\n",
      "\n",
      "**In essence, the joke is funny because it:**\n",
      "\n",
      "*   **Subverts expectations:** It takes a familiar joke structure and applies it to a modern, complex topic like AI.\n",
      "*   **Relies on AI concepts:** It uses common AI terms (algorithm, prediction, dataset) to create a humorous situation.\n",
      "*   **Personifies AI:** It gives the AI a (simplified and exaggerated) motivation based on its programming: accuracy and data acquisition.\n",
      "*   **Is relatable:** The cat picture detail is a humorous nod to the vast amounts of data that AI models are trained on, making it relatable to anyone who knows a little about AI.\n",
      "\n",
      "The humor comes from the contrast between the simple, almost absurd premise (crossing the road) and the complex, data-driven explanation provided for the AI's actions. It pokes fun at the way AI is often portrayed as being driven by logic and data, sometimes to an almost comical degree.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
    "\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974839f",
   "metadata": {},
   "source": [
    "#### Runnable Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7729481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is rapidly evolving, and it's both exciting and a little daunting. What are your thoughts on the future of AI and its impact on society? #AI #ArtificialIntelligence #FutureTech #Innovation #Technology\n",
      "## Option 1: Focusing on the Impact of AI\n",
      "\n",
      "**Headline:** AI: More Than Just Hype - Transforming Industries and Creating Opportunities\n",
      "\n",
      "**Body:**\n",
      "\n",
      "We hear a lot about AI these days, but it's important to look beyond the buzz and understand the real impact it's having across industries. From automating tasks and improving efficiency to driving innovation and creating entirely new business models, AI is revolutionizing the way we work and live.\n",
      "\n",
      "I'm particularly excited about [mention a specific area of AI you're interested in, e.g., its potential in healthcare, its impact on customer service, or its role in sustainable development]. \n",
      "\n",
      "What are your thoughts on the most transformative aspects of AI? Let's discuss! #AI #ArtificialIntelligence #Innovation #Technology #FutureofWork #DigitalTransformation\n",
      "\n",
      "**Image:** A compelling image related to AI, such as robots working alongside humans, data visualization, or a futuristic cityscape.\n",
      "\n",
      "## Option 2: Highlighting AI Skills and Learning\n",
      "\n",
      "**Headline:** Level Up Your Skills: The Growing Demand for AI Expertise\n",
      "\n",
      "**Body:**\n",
      "\n",
      "In today's rapidly evolving job market, AI skills are becoming increasingly valuable. Whether you're a developer, marketer, or project manager, understanding the fundamentals of AI can significantly enhance your career prospects.\n",
      "\n",
      "I've been exploring [mention a specific AI learning resource you've used, e.g., online courses, workshops, or books] and found it incredibly insightful. \n",
      "\n",
      "What resources have you found helpful in learning about AI? Share your recommendations in the comments! #AI #ArtificialIntelligence #Skills #Learning #Education #CareerDevelopment #FutureSkills\n",
      "\n",
      "**Image:** An image related to learning about AI, such as someone using a laptop to study AI, a classroom setting, or a graphic representing AI concepts.\n",
      "\n",
      "## Option 3: Sharing an AI-Related Article or Resource\n",
      "\n",
      "**Headline:** Interesting Read: [Title of the Article/Resource] - [Brief, Engaging Summary]\n",
      "\n",
      "**Body:**\n",
      "\n",
      "Just came across this insightful article/resource on [topic related to AI]. [Provide a brief summary of the key takeaways and why you found it valuable].\n",
      "\n",
      "[Link to the article/resource]\n",
      "\n",
      "This really got me thinking about [mention a specific point or question the article/resource raised]. What are your thoughts on this? #AI #ArtificialIntelligence #MachineLearning #DataScience #Innovation #Technology #[Related Keyword]\n",
      "\n",
      "**Image:** A relevant image from the article or a general image related to the topic.\n",
      "\n",
      "**Remember to:**\n",
      "\n",
      "*   **Tailor the post to your specific audience and interests.**\n",
      "*   **Use relevant hashtags to increase visibility.**\n",
      "*   **Engage with comments and participate in the discussion.**\n",
      "\n",
      "Good luck!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a tweet about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a Linkedin post about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'tweet': RunnableSequence(prompt1, model, parser),\n",
    "    'linkedin': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({'topic':'AI'})\n",
    "\n",
    "print(result['tweet'])\n",
    "print(result['linkedin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142024",
   "metadata": {},
   "source": [
    "#### Runnable Passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890de506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the cricket team bring a ladder to the game?\\n\\nBecause they heard the other team had a really high score to chase!', 'explanation': 'The joke plays on the double meaning of the word \"chase\" in cricket:\\n\\n* **Literal Meaning:** A ladder is used to reach high places.\\n* **Cricket Meaning:** In cricket, when one team bats first and sets a target score, the second team has to \"chase\" that score to win.  They need to score enough runs to surpass the first team\\'s total.\\n\\nThe joke is funny because it creates a humorous misunderstanding. The cricket team heard they had a \"high score to chase\" and misinterpreted it literally, thinking they needed a ladder to physically reach the high score.  The humor comes from the silly image of cricketers bringing a ladder to a game to \"chase\" runs in that way.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'cricket'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5aa790",
   "metadata": {},
   "source": [
    "#### Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd879120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road?\n",
      "\n",
      "Because the algorithm said there was a 99.99% probability it would reach the other side. Turns out, it just needed to fetch more training data. \n",
      " word count - 32\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'word_count': RunnableLambda(word_count)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({'topic':'AI'})\n",
    "\n",
    "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0de9f",
   "metadata": {},
   "source": [
    "#### Runnable Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fc0876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Russia-Ukraine conflict is a complex crisis rooted in a long history of shared heritage, periods of domination, and Soviet influence. Key events leading to the 2022 invasion include NATO expansion, the Orange and Euromaidan Revolutions, Russia's annexation of Crimea, and the war in Donbas. Russia launched a full-scale invasion in 2022, citing goals of \"demilitarization\" and \"denazification,\" leading to widespread condemnation, sanctions, and military/humanitarian aid to Ukraine from the international community. Key actors include the leaders of Russia, Ukraine, the US, the EU, and NATO. The conflict continues with fighting concentrated in eastern and southern Ukraine, and international support for Ukraine facing some concerns about long-term sustainability. Potential outcomes range from a protracted conflict to a negotiated settlement or a victory for either side, with long-term implications for European security, Russia-West relations, the global order, and the global economy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea440f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
